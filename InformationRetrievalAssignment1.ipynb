{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRaUCSACcMuGWg9RPtscXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swanand58/intro-to-information-retrieval/blob/main/InformationRetrievalAssignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_XyIkJ96_z",
        "outputId": "9a658aa7-88ef-4813-8626-68495704707b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_lists = [\n",
        "    \"Today OpenAI released ChatGPT!!!!!! Go ChatGPT!\",\n",
        "    \"The ChatGPT is released today.\",\n",
        "    \"OpenAI's ChatGPT performed well.\",\n",
        "    \"Find the ChatGPT news last week.\",\n",
        "    \"ChatGPT is a system released by OpenAI.\"\n",
        "]"
      ],
      "metadata": {
        "id": "5COD1Ozb-MHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "-ABL5kbU-RED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(document):\n",
        "\n",
        "  print(\"Original Given Document: \", document)\n",
        "\n",
        "  #Case folding - We will convert all the words in the documents to lower case.\n",
        "  document = document.lower()\n",
        "  print(\"Case Folding Removal: \",document)\n",
        "\n",
        "  #Punctuation Removal step - We will strip all the punctuation marks from the sentences of the documents with the help of string.punctuation.\n",
        "  document = document.translate(str.maketrans('', '', string.punctuation))\n",
        "  print(\"Punctuation Removal Output: \", document)\n",
        "\n",
        "  #Converting into tokens\n",
        "  tokens = word_tokenize(document)\n",
        "  print(\"Token output: \",tokens)\n",
        "\n",
        "  #Stop word removal - We will remove all stop word using nltk library predefined list of english stop words.\n",
        "  filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "  print(\"Stop word removal output: \", filtered_tokens)\n",
        "\n",
        "  #Lemmatizing the Tokens using WordNetLemmatizer\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "  print(\"Lemmatization output: \", lemmatized_tokens)\n",
        "\n",
        "  #Stemming the tokens using PorterStemmer\n",
        "  stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
        "  print(\"Stemming output: \", stemmed_tokens)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return stemmed_tokens"
      ],
      "metadata": {
        "id": "D62Dw7fu-YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_docs = [preprocess(document) for document in document_lists]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulw7UCl9fIl4",
        "outputId": "2a2dcaa4-1b33-4a9f-be43-a6981e9fce08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Given Document:  Today OpenAI released ChatGPT!!!!!! Go ChatGPT!\n",
            "Case Folding Removal:  today openai released chatgpt!!!!!! go chatgpt!\n",
            "Punctuation Removal Output:  today openai released chatgpt go chatgpt\n",
            "Token output:  ['today', 'openai', 'released', 'chatgpt', 'go', 'chatgpt']\n",
            "Stop word removal output:  ['today', 'openai', 'released', 'chatgpt', 'go', 'chatgpt']\n",
            "Lemmatization output:  ['today', 'openai', 'released', 'chatgpt', 'go', 'chatgpt']\n",
            "Stemming output:  ['today', 'openai', 'releas', 'chatgpt', 'go', 'chatgpt']\n",
            "\n",
            "\n",
            "Original Given Document:  The ChatGPT is released today.\n",
            "Case Folding Removal:  the chatgpt is released today.\n",
            "Punctuation Removal Output:  the chatgpt is released today\n",
            "Token output:  ['the', 'chatgpt', 'is', 'released', 'today']\n",
            "Stop word removal output:  ['chatgpt', 'released', 'today']\n",
            "Lemmatization output:  ['chatgpt', 'released', 'today']\n",
            "Stemming output:  ['chatgpt', 'releas', 'today']\n",
            "\n",
            "\n",
            "Original Given Document:  OpenAI's ChatGPT performed well.\n",
            "Case Folding Removal:  openai's chatgpt performed well.\n",
            "Punctuation Removal Output:  openais chatgpt performed well\n",
            "Token output:  ['openais', 'chatgpt', 'performed', 'well']\n",
            "Stop word removal output:  ['openais', 'chatgpt', 'performed', 'well']\n",
            "Lemmatization output:  ['openais', 'chatgpt', 'performed', 'well']\n",
            "Stemming output:  ['openai', 'chatgpt', 'perform', 'well']\n",
            "\n",
            "\n",
            "Original Given Document:  Find the ChatGPT news last week.\n",
            "Case Folding Removal:  find the chatgpt news last week.\n",
            "Punctuation Removal Output:  find the chatgpt news last week\n",
            "Token output:  ['find', 'the', 'chatgpt', 'news', 'last', 'week']\n",
            "Stop word removal output:  ['find', 'chatgpt', 'news', 'last', 'week']\n",
            "Lemmatization output:  ['find', 'chatgpt', 'news', 'last', 'week']\n",
            "Stemming output:  ['find', 'chatgpt', 'news', 'last', 'week']\n",
            "\n",
            "\n",
            "Original Given Document:  ChatGPT is a system released by OpenAI.\n",
            "Case Folding Removal:  chatgpt is a system released by openai.\n",
            "Punctuation Removal Output:  chatgpt is a system released by openai\n",
            "Token output:  ['chatgpt', 'is', 'a', 'system', 'released', 'by', 'openai']\n",
            "Stop word removal output:  ['chatgpt', 'system', 'released', 'openai']\n",
            "Lemmatization output:  ['chatgpt', 'system', 'released', 'openai']\n",
            "Stemming output:  ['chatgpt', 'system', 'releas', 'openai']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ACYOXqkO_J",
        "outputId": "17aa35df-776b-4bc6-e4bd-0b0c0d7ab503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['today', 'openai', 'releas', 'chatgpt', 'go', 'chatgpt'], ['chatgpt', 'releas', 'today'], ['openai', 'chatgpt', 'perform', 'well'], ['find', 'chatgpt', 'news', 'last', 'week'], ['chatgpt', 'system', 'releas', 'openai']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# we will get all the unique terms from the preprocessed documents list\n",
        "unique_terms = sorted(set(term for doc in preprocessed_docs for term in doc))\n",
        "\n",
        "#we will initialize a matrix and fill 0 in the matrix\n",
        "incidence_matrix = pd.DataFrame(0, index=unique_terms, columns=[f\"Document {i+1}\" for i in range(len(preprocessed_docs))])\n",
        "\n",
        "#we will construct the incidence matrix and add 1 where the word occurs\n",
        "for i, doc in enumerate(preprocessed_docs):\n",
        "    for term in doc:\n",
        "        incidence_matrix.at[term, f\"Document {i+1}\"] = 1\n",
        "\n",
        "print(\"Incidence Matrix is as follows:\")\n",
        "print(\"\\n\")\n",
        "print(incidence_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhYn60ONj1se",
        "outputId": "79a35337-1708-4e3c-bc50-fb579f685f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incidence Matrix is as follows:\n",
            "\n",
            "\n",
            "         Document 1  Document 2  Document 3  Document 4  Document 5\n",
            "chatgpt           1           1           1           1           1\n",
            "find              0           0           0           1           0\n",
            "go                1           0           0           0           0\n",
            "last              0           0           0           1           0\n",
            "news              0           0           0           1           0\n",
            "openai            1           0           1           0           1\n",
            "perform           0           0           1           0           0\n",
            "releas            1           1           0           0           1\n",
            "system            0           0           0           0           1\n",
            "today             1           1           0           0           0\n",
            "week              0           0           0           1           0\n",
            "well              0           0           1           0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing TF-IDF on the term document incident matrix\n",
        "# We know that the idf of t is given by log(N/df_t) Where N is the total number of documents and df_t is the document frequency of term t\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "N = len(preprocessed_docs)\n",
        "\n",
        "idf_values = np.log(N / incidence_matrix.sum(axis=1))\n",
        "\n",
        "tf_idf_matrix = incidence_matrix.mul(idf_values, axis=0)\n",
        "\n",
        "print(tf_idf_matrix)"
      ],
      "metadata": {
        "id": "OFxHXkZMk_y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d84d73f-ce0e-4002-dbb0-88dfe73881a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Document 1  Document 2  Document 3  Document 4  Document 5\n",
            "chatgpt    0.000000    0.000000    0.000000    0.000000    0.000000\n",
            "find       0.000000    0.000000    0.000000    1.609438    0.000000\n",
            "go         1.609438    0.000000    0.000000    0.000000    0.000000\n",
            "last       0.000000    0.000000    0.000000    1.609438    0.000000\n",
            "news       0.000000    0.000000    0.000000    1.609438    0.000000\n",
            "openai     0.510826    0.000000    0.510826    0.000000    0.510826\n",
            "perform    0.000000    0.000000    1.609438    0.000000    0.000000\n",
            "releas     0.510826    0.510826    0.000000    0.000000    0.510826\n",
            "system     0.000000    0.000000    0.000000    0.000000    1.609438\n",
            "today      0.916291    0.916291    0.000000    0.000000    0.000000\n",
            "week       0.000000    0.000000    0.000000    1.609438    0.000000\n",
            "well       0.000000    0.000000    1.609438    0.000000    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(tf_idf_matrix.T)\n",
        "\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=[f\"Document {i+1}\" for i in range(5)], columns=[f\"Document {i+1}\" for i in range(5)])\n",
        "\n",
        "print(\"Cosine Similarity:\")\n",
        "print(\"\\n\")\n",
        "print(cosine_sim_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IijTqH4clQsI",
        "outputId": "073f1820-ea63-49dd-cf52-1dcd22d20946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity:\n",
            "\n",
            "\n",
            "            Document 1  Document 2  Document 3  Document 4  Document 5\n",
            "Document 1    1.000000    0.527723    0.056272         0.0    0.148815\n",
            "Document 2    0.527723    1.000000    0.000000         0.0    0.140998\n",
            "Document 3    0.056272    0.000000    1.000000         0.0    0.063409\n",
            "Document 4    0.000000    0.000000    0.000000         1.0    0.000000\n",
            "Document 5    0.148815    0.140998    0.063409         0.0    1.000000\n"
          ]
        }
      ]
    }
  ]
}